{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607bdbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703b0585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f5df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3071ab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    'google/flan-t5-base',\n",
    "    skip_chat_template=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e443c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parents are being asked to consider whether video games are suitable for children.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"summarize: Parents face video game lessons Ways of ensuring that parents know which video games are suitable for children are to be considered by the games industry.The issue was discussed at a meeting between UK government officials, industry representatives and the British Board of Film Classification. It follows concerns that children may be playing games aimed at adults which include high levels of violence. In 2003, Britons spent Â£1,152m on games, more than ever before. And this Christmas, parents are expected to spend millions on video games and consoles.\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99a4b3",
   "metadata": {},
   "source": [
    "## Read and combine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771576dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2225\n",
      "Categories: ['business' 'entertainment' 'politics' 'sport' 'tech']\n",
      "   category                                            article  \\\n",
      "0  business  Ad sales boost Time Warner profit\\n\\nQuarterly...   \n",
      "1  business  Dollar gains on Greenspan speech\\n\\nThe dollar...   \n",
      "2  business  Yukos unit buyer faces loan claim\\n\\nThe owner...   \n",
      "3  business  High fuel prices hit BA's profits\\n\\nBritish A...   \n",
      "4  business  Pernod takeover talk lifts Domecq\\n\\nShares in...   \n",
      "\n",
      "                                             summary  \n",
      "0  TimeWarner said fourth quarter sales rose 2% t...  \n",
      "1  The dollar has hit its highest level against t...  \n",
      "2  Yukos' owner Menatep Group says it will ask Ro...  \n",
      "3  Rod Eddington, BA's chief executive, said the ...  \n",
      "4  Pernod has reduced the debt it took on to fund...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_path = \"BBC News Summary\" \n",
    "\n",
    "articles_dir = os.path.join(base_path, \"News Articles\")\n",
    "summaries_dir = os.path.join(base_path, \"Summaries\")\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through categories (business, politics, sport, etc.)\n",
    "for category in os.listdir(articles_dir):\n",
    "    article_cat_dir = os.path.join(articles_dir, category)\n",
    "    summary_cat_dir = os.path.join(summaries_dir, category)\n",
    "    \n",
    "    for fname in os.listdir(article_cat_dir):\n",
    "        article_path = os.path.join(article_cat_dir, fname)\n",
    "        summary_path = os.path.join(summary_cat_dir, fname)\n",
    "        \n",
    "        if os.path.exists(article_path) and os.path.exists(summary_path):\n",
    "            \n",
    "            with open(article_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                article = f.read().strip()\n",
    "            with open(summary_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                summary = f.read().strip()\n",
    "            \n",
    "            data.append({\n",
    "                \"category\": category,\n",
    "                \"article\": article,\n",
    "                \"summary\": summary\n",
    "            })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"Categories:\", df['category'].unique())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d21183",
   "metadata": {},
   "source": [
    "## Clean and inspect text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "299f9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average article length: 425.78426966292136\n",
      "Average summary length: 189.6485393258427\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>politics</td>\n",
       "      <td>Labour accused of broken pledge Labour has alr...</td>\n",
       "      <td>Home Secretary Charles Clarke has been quoted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>politics</td>\n",
       "      <td>Borders rail link campaign rally Campaigners a...</td>\n",
       "      <td>Anne Borthwick , of Campaign for Borders Rail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>politics</td>\n",
       "      <td>Parties build up poll war chests The Labour Pa...</td>\n",
       "      <td>Registered political parties are required to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>tech</td>\n",
       "      <td>A decade of good website design The web looks ...</td>\n",
       "      <td>Dr Nielsen said the success of sites such as G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>sport</td>\n",
       "      <td>England claim Dubai Sevens glory England beat ...</td>\n",
       "      <td>England beat Fiji 26-21 in a dramatic final in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                            article  \\\n",
       "1042  politics  Labour accused of broken pledge Labour has alr...   \n",
       "933   politics  Borders rail link campaign rally Campaigners a...   \n",
       "1217  politics  Parties build up poll war chests The Labour Pa...   \n",
       "1991      tech  A decade of good website design The web looks ...   \n",
       "1693     sport  England claim Dubai Sevens glory England beat ...   \n",
       "\n",
       "                                                summary  \n",
       "1042  Home Secretary Charles Clarke has been quoted ...  \n",
       "933   Anne Borthwick , of Campaign for Borders Rail ...  \n",
       "1217  Registered political parties are required to s...  \n",
       "1991  Dr Nielsen said the success of sites such as G...  \n",
       "1693  England beat Fiji 26-21 in a dramatic final in...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = re.sub(r'([.,!?;:])', r' \\1 ', text)  \n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df[\"article\"] = df[\"article\"].apply(clean_text)\n",
    "df[\"summary\"] = df[\"summary\"].apply(clean_text)\n",
    "\n",
    "print(\"Average article length:\", df[\"article\"].apply(lambda x: len(x.split())).mean())\n",
    "print(\"Average summary length:\", df[\"summary\"].apply(lambda x: len(x.split())).mean())\n",
    "\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab035ef",
   "metadata": {},
   "source": [
    "## Split into train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "263181af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1801, Val: 201, Test: 223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b332694",
   "metadata": {},
   "source": [
    "## Save cleaned data for later stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "val_df.to_csv(\"data/val.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e600274",
   "metadata": {},
   "source": [
    "## Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25222ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CATEGORY: tech\n",
      "ARTICLE: Norway upholds 'Napster' ruling A Norwegian student who ran a website which linked to downloadable MP3 files has been ordered to pay compensation by the country's Supreme Court . Frank Allan Bruvik was ordered to pay 100 , 000 kroner (Â£8 , 000) to the music industry in Norway . He was a student when he set up his napster . no site , which allowed users to submit and receive links to MP3 files . Bruvik had earlier been cleared on appeal after a lower court had found for the music industry . Music ...\n",
      "SUMMARY: Frank Allan Bruvik was ordered to pay 100 , 000 kroner (Â£8 , 000) to the music industry in Norway . Norway's music industry said it was satisfied with the ruling , because showed that music piracy would not be accepted . A Norwegian court ruled in 2003 that Bruvik would have to pay 100 , 000 kroner to the music industry , but the country's Court of Appeal cleared him , saying that the copyright violation occurred when others posted the music . Bruvik's site was online between August and November 2001 , and while it did not host any music , at its peak it was providing links to more than 170 free files on other servers . However , the Supreme Court stated that the music was clearly published in violation of copyright law It added that the case was decided based on the responsibility for abetting an illegal act , and that Bruvik's actions were premeditated . A legal complaint for copyright violation was filed by groups including Norway's performing rights society , Tono , and the Norwegian branches of Sony Music and Universal Music , who saw it as an important test of principle . Bruvik had earlier been cleared on appeal after a lower court had found for the music industry .\n",
      "\n",
      "CATEGORY: sport\n",
      "ARTICLE: Saint-Andre anger at absent stars Sale Sharks director of rugby Philippe Saint-Andre has re-opened rugby's club-versus-country debate . Sale host Bath in the Powergen Cup on Friday , but the Frenchman has endured a \"difficult week\" with six players away on England's Six Nations training camp . \"It's an important game but we've just the one full session . It's the same for everyone but we need to manage it . \"If five players or more are picked for your country they should move the date of the gam ...\n",
      "SUMMARY: \"They obviously want him for international rugby and we want him for club rugby in what is a very important game for us . Sale host Bath in the Powergen Cup on Friday , but the Frenchman has endured a \"difficult week\" with six players away on England's Six Nations training camp . \"Under the Elite Player Squad agreement , England wanted a second opinion , which they can do , \" director of rugby Nigel Melville told the Gloucester Citizen . \"Sebastien Bruno's been with France , Jason White with Scotland and there are six with England , that's eight players plus injuries - 13 players out of a squad of 31 . \"It's an important game but we've just the one full session . Simpson-Daniel added : \"I've said to Nigel I want to be back playing and that means if everything goes well this week , I can target the Worcester game (on 29 January) for a return . \"Gloucester have also been caught in a club-versus-country conflict after England sought a second medical opinion on James Simpson-Daniel's fitness . Sale Sharks director of rugby Philippe Saint-Andre has re-opened rugby's club-versus-country debate . Bath have just been knocked out of Europe and will make it a tough game . \"If five players or more are picked for your country they should move the date of the game , \" he told BBC Sport .\n",
      "\n",
      "CATEGORY: business\n",
      "ARTICLE: Bank opts to leave rates on hold The Bank of England has left interest rates on hold at 4 . 75% for a sixth month in a row . The Bank's Monetary Policy Committee (MPC) decided to take no action amid mixed signals from the economy . But some economists predict a further rise in the cost of borrowing will come later this year . Interest rates rose five times between November 2003 and August 2004 as soaring house prices and buoyant consumer data sparked inflation fears . Bank of England governor Me ...\n",
      "SUMMARY: Slower consumer spending and a quieter housing market are likely to have convinced the MPC that rates should be left unchanged in recent months . Howard Archer , economist at Global Insight said the MPC \"may well consider that the balance of risks to the growth and inflation outlook have moved from the downside to the upside\" . Economists see next week's inflation report from the MPC as key in assessing whether a further interest rate rise is necessary to keep the economy in check . Interest rates rose five times between November 2003 and August 2004 as soaring house prices and buoyant consumer data sparked inflation fears . Philip Shaw , chief economist at Investec Securities , said he believed rates would remain at 4 . 75% for the rest of the year although strong economic data could lead to a further hike . The British Chambers of Commerce (BCC) said it was \"concerned by the clamour in some quarters\" for early interest rate increases . The Bank of England has left interest rates on hold at 4 . 75% for a sixth month in a row . Data on Wednesday also showed manufacturing output rose at its fastest rate since May last month , reinforcing a view that economic growth was stronger than forecasts . \"Growth appears stronger and the MPC may become more concerned about inflation trends . \"\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\nCATEGORY: {train_df.iloc[i]['category']}\")\n",
    "    print(\"ARTICLE:\", train_df.iloc[i]['article'][:500], \"...\")\n",
    "    print(\"SUMMARY:\", train_df.iloc[i]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8605d12",
   "metadata": {},
   "source": [
    "## Prepare for tokenization (T5/BART) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf12bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name_t5 = \"google/flan-t5-base\"\n",
    "model_name_bart = \"facebook/bart-base\"\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(model_name_t5)\n",
    "tokenizer_bart = AutoTokenizer.from_pretrained(model_name_bart)\n",
    "\n",
    "# Example tokenization test\n",
    "sample = train_df.iloc[0][\"article\"]\n",
    "inputs = tokenizer_t5(\"summarize: \" + sample, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90adff7d",
   "metadata": {},
   "source": [
    "## Tokenize and prepare Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "896200c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|ââââââââââ| 1801/1801 [00:00<00:00, 2227.27 examples/s]\n",
      "Map: 100%|ââââââââââ| 201/201 [00:00<00:00, 2777.13 examples/s]\n",
      "Map: 100%|ââââââââââ| 223/223 [00:00<00:00, 2752.43 examples/s]\n",
      "Map: 100%|ââââââââââ| 1801/1801 [00:00<00:00, 2927.58 examples/s]\n",
      "Map: 100%|ââââââââââ| 201/201 [00:00<00:00, 2991.25 examples/s]\n",
      "Map: 100%|ââââââââââ| 223/223 [00:00<00:00, 2593.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 sample input_ids: [21603, 10, 16491, 95, 6134, 7, 3, 31, 567, 9]\n",
      "BART sample input_ids: [0, 29723, 1970, 16060, 29, 128, 37549, 3121, 108, 2255]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "#Load CSVs into Hugging Face Datasets\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/val.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Define tokenization function\n",
    "\n",
    "def tokenize_function_t5(batch):\n",
    "    inputs = [\"summarize: \" + text for text in batch[\"article\"]]\n",
    "    model_inputs = tokenizer_t5(inputs, max_length=512, truncation=True)\n",
    "    labels = tokenizer_t5(batch[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def tokenize_function_bart(batch):\n",
    "    inputs = batch[\"article\"]\n",
    "    model_inputs = tokenizer_bart(inputs, max_length=512, truncation=True)\n",
    "    labels = tokenizer_bart(batch[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "#  Apply tokenization\n",
    "\n",
    "tokenized_train_t5 = train_ds.map(tokenize_function_t5, batched=True)\n",
    "tokenized_val_t5 = val_ds.map(tokenize_function_t5, batched=True)\n",
    "tokenized_test_t5 = test_ds.map(tokenize_function_t5, batched=True)\n",
    "\n",
    "tokenized_train_bart = train_ds.map(tokenize_function_bart, batched=True)\n",
    "tokenized_val_bart = val_ds.map(tokenize_function_bart, batched=True)\n",
    "tokenized_test_bart = test_ds.map(tokenize_function_bart, batched=True)\n",
    "\n",
    "# Quick sanity check\n",
    "\n",
    "print(\"T5 sample input_ids:\", tokenized_train_t5[0][\"input_ids\"][:10])\n",
    "print(\"BART sample input_ids:\", tokenized_train_bart[0][\"input_ids\"][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d3c2f",
   "metadata": {},
   "source": [
    "## Fine-tuning setup with LoRA for T5 and BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e69a4dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ð¤ Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ð¤ Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5:\n",
      "Trainable params: 1,769,472 / 249,347,328 (0.71%)\n",
      "BART:\n",
      "Trainable params: 884,736 / 140,305,152 (0.63%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "\n",
    "#  Load base models\n",
    "\n",
    "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model_bart = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "\n",
    "# Define LoRA configuration\n",
    "\n",
    "lora_config_t5 = LoraConfig(\n",
    "    r=16,              # rank (usually 8â32)\n",
    "    lora_alpha=32,     # scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # which layers to adapt (for seq2seq models)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "lora_config_bart = LoraConfig(\n",
    "    r=16,              # rank (usually 8â32)\n",
    "    lora_alpha=32,     # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which layers to adapt (for seq2seq models)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "#  Wrap models with LoRA adapters\n",
    "\n",
    "model_t5 = get_peft_model(model_t5, lora_config_t5)\n",
    "model_bart = get_peft_model(model_bart, lora_config_bart)\n",
    "\n",
    "#  Training arguments\n",
    "\n",
    "training_args_t5 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_t5\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs_t5\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),  \n",
    ")\n",
    "\n",
    "training_args_bart = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_bart\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs_bart\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n",
    "#  Data collators\n",
    "\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(tokenizer_t5, model=model_t5)\n",
    "data_collator_bart = DataCollatorForSeq2Seq(tokenizer_bart, model=model_bart)\n",
    "\n",
    "\n",
    "#  Define trainers\n",
    "\n",
    "trainer_t5 = Seq2SeqTrainer(\n",
    "    model=model_t5,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=tokenized_train_t5,\n",
    "    eval_dataset=tokenized_val_t5,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator_t5,\n",
    ")\n",
    "\n",
    "trainer_bart = Seq2SeqTrainer(\n",
    "    model=model_bart,\n",
    "    args=training_args_bart,\n",
    "    train_dataset=tokenized_train_bart,\n",
    "    eval_dataset=tokenized_val_bart,\n",
    "    tokenizer=tokenizer_bart,\n",
    "    data_collator=data_collator_bart,\n",
    ")\n",
    "\n",
    "\n",
    "#  Print trainable parameters \n",
    "\n",
    "def print_trainable_params(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "print(\"T5:\")\n",
    "print_trainable_params(model_t5)\n",
    "print(\"BART:\")\n",
    "print_trainable_params(model_bart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469fe91",
   "metadata": {},
   "source": [
    "## Train and Evaluate LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cce292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning T5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1353' max='1353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1353/1353 13:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>18.331700</td>\n",
       "      <td>10.863600</td>\n",
       "      <td>15.904300</td>\n",
       "      <td>17.497800</td>\n",
       "      <td>18.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>18.331700</td>\n",
       "      <td>10.863600</td>\n",
       "      <td>15.904300</td>\n",
       "      <td>17.497800</td>\n",
       "      <td>18.328400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>18.331700</td>\n",
       "      <td>10.863600</td>\n",
       "      <td>15.904300</td>\n",
       "      <td>17.497800</td>\n",
       "      <td>18.328400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BART...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1353' max='1353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1353/1353 06:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>0.399232</td>\n",
       "      <td>17.566600</td>\n",
       "      <td>14.404800</td>\n",
       "      <td>17.097000</td>\n",
       "      <td>17.382300</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.374040</td>\n",
       "      <td>18.409700</td>\n",
       "      <td>15.478400</td>\n",
       "      <td>17.924500</td>\n",
       "      <td>18.274100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.361583</td>\n",
       "      <td>18.931200</td>\n",
       "      <td>15.985500</td>\n",
       "      <td>18.365500</td>\n",
       "      <td>18.770100</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/bouaz/Desktop/FineTuning_project/env/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating T5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 01:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_rouge1': 18.3317, 'eval_rouge2': 10.8636, 'eval_rougeL': 15.9043, 'eval_rougeLsum': 17.4978, 'eval_gen_len': 18.3284, 'eval_runtime': 65.9953, 'eval_samples_per_second': 3.046, 'eval_steps_per_second': 0.773, 'epoch': 3.0}\n",
      "Evaluating BART...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36158257722854614, 'eval_rouge1': 18.9312, 'eval_rouge2': 15.9855, 'eval_rougeL': 18.3655, 'eval_rougeLsum': 18.7701, 'eval_gen_len': 19.0, 'eval_runtime': 40.4132, 'eval_samples_per_second': 4.974, 'eval_steps_per_second': 1.262, 'epoch': 3.0}\n",
      "\n",
      "ð° Original article:\n",
      " UK house prices dip in November UK house prices dipped slightly in November , the Office of the Deputy Prime Minister (ODPM) has said . The average house price fell marginally to Â£180 , 226 , from Â£180 , 444 in October . Recent evidence has suggested that the UK housing market is slowing after interest rate increases , and economists forecast a drop in prices during 2005 . But while the monthly figures may hint at a cooling of the market , annual house price inflation is still strong , up 13 . 8 ...\n",
      "\n",
      "ð§¾ Reference summary:\n",
      " All areas saw a rise in annual house price inflation in November except for Northern Ireland and the West Midlands , where the rate was unchanged , the ODPM said . It said annual inflation rose between October and November because prices had fallen by 1 . 1% in the same period in 2003 . In London , the area with the highest average house price at Â£262 , 825 , annual inflation rose only slightly in November to 7 . 1% from 7% the previous month . UK house prices dipped slightly in November , the Office of the Deputy Prime Minister (ODPM) has said . But while the monthly figures may hint at a cooling of the market , annual house price inflation is still strong , up 13 . 8% in the year to November . The ODPM attributed the monthly fall of prices in November to a drop in the value of detached houses and flats . The ODPM data showed the average house price was Â£192 , 713 in England ; Â£139 , 544 in Wales ; Â£116 , 542 in Scotland , and Â£111 , 314 in Northern Ireland . The average house price fell marginally to Â£180 , 226 , from Â£180 , 444 in October .\n",
      "\n",
      "ð¤ T5 summary:\n",
      " UK house prices dipped slightly in November, Office of the Deputy Prime Minister (ODPM) has said. The average house price fell marginally to Â£180, 226 from Â£180, 444 in October. Recent evidence has suggested that the UK housing market is slowing after interest rate increases, and economists forecast a drop in prices during 2005. But while the monthly figures may hint at a cooling of the market, annual house price inflation is still strong, up 13. 8% in the year to November.\n",
      "\n",
      "ð¤ BART summary:\n",
      " UK house prices dipped slightly in November, the Office of the Deputy Prime Minister (ODPM) has said. The ODPM attributed the monthly fall of prices in November to a drop in the value of detached houses and flats. In London, the area with the highest average house price at Â£192, 713 in England ; Â£139, 544 in Wales ; Â£116, 542 in Scotland, and Â£111, 314 in Northern Ireland. But while the monthly figures may hint at a cooling of the market, annual house price inflation is still strong, up 13. 8% in the year to November. The North East\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load ROUGE metric\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id to make decoding safe\n",
    "    labels = np.where(labels == -100, tokenizer_t5.pad_token_id, labels)\n",
    "\n",
    "    # Decode predictions and labels safely\n",
    "    decoded_preds = tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Post-process: clean up spacing and ensure valid strings\n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split(\". \")) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split(\". \")) for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer_t5.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "\n",
    "#  Attach metrics to trainer\n",
    "\n",
    "trainer_t5.compute_metrics = compute_metrics\n",
    "trainer_bart.compute_metrics = compute_metrics\n",
    "\n",
    "\n",
    "#  Train models \n",
    "\n",
    "print(\"Fine-tuning T5...\")\n",
    "trainer_t5.train()\n",
    "\n",
    "print(\"Fine-tuning BART...\")\n",
    "trainer_bart.train()\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "\n",
    "print(\"Evaluating T5...\")\n",
    "metrics_t5 = trainer_t5.evaluate()\n",
    "print(metrics_t5)\n",
    "\n",
    "print(\"Evaluating BART...\")\n",
    "metrics_bart = trainer_bart.evaluate()\n",
    "print(metrics_bart)\n",
    "\n",
    "\n",
    "#  Generate sample summaries\n",
    "\n",
    "def generate_summary(model, tokenizer, text, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_length, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_text = test_df.iloc[0][\"article\"]\n",
    "\n",
    "print(\"\\nð° Original article:\\n\", sample_text[:500], \"...\")\n",
    "print(\"\\nð§¾ Reference summary:\\n\", test_df.iloc[0][\"summary\"])\n",
    "print(\"\\nð¤ T5 summary:\\n\", generate_summary(model_t5, tokenizer_t5, \"summarize: \" + sample_text))\n",
    "print(\"\\nð¤ BART summary:\\n\", generate_summary(model_bart, tokenizer_bart, sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f78f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
